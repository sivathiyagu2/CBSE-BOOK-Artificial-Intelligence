Unit 1 INTRODUCTION TO AI

Project 1: Creating a Basic Chatbot

A chatbot is an AI application designed to simulate conversation with human users. This project will guide you through creating a basic rule-based chatbot using Python.
Step-by-Step Guide:
Set Up the Environment:
Install Python and a text editor or IDE (e.g., Visual Studio Code).


pip install nltk
import nltk
from nltk.chat.util import Chat, reflections

pairs = [
    [
        r"my name is (.*)",
        ["Hello %1, How can I help you today?",]
    ],
    [
        r"hi|hello|hey",
        ["Hello!", "Hey there!",]
    ],
    [
        r"what is your name?",
        ["I am a chatbot created to assist you.",]
    ],
    [
        r"how are you?",
        ["I'm doing good. How about you?",]
    ],
    [
        r"sorry (.*)",
        ["It's okay.", "No problem at all.",]
    ],
    [
        r"quit",
        ["Bye! Take care.",]
    ],
]


def chatbot():
    print("Hi! I am your chatbot. Type 'quit' to exit.")
    chat = Chat(pairs, reflections)
    chat.converse()

if __name__ == "__main__":
    chatbot()

python chatbot.py


Project 2: Image Recognition Using Pretrained Models
Image recognition involves identifying objects or features within an image. This project uses a pretrained model to classify images.

Step-by-Step Guide:

pip install tensorflow keras numpy matplotlib

Import Libraries:

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'  # Replace with the path to your image
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:

plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()

Run the Script:
python image_recognition.py


Exercise 1: Data Cleaning
Objective: Learn how to clean and preprocess a dataset using Python and pandas.

pip install pandas numpy

Import Libraries:
import pandas as pd
import numpy as np

Load the Dataset:
df = pd.read_csv('path_to_your_dataset.csv')  # Replace with the path to your dataset

Inspect the Dataset:
print(df.head())
print(df.info())

Handle Missing Values

Identify Missing Values:
print(df.isnull().sum())

Fill Missing Values:
df.fillna(df.mean(), inplace=True)

Drop Rows with Missing Values:
df.dropna(inplace=True)

Normalize Data:
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
df_scaled = pd.DataFrame(df_scaled, columns=df.columns)

Save the Cleaned Dataset:
df_scaled.to_csv('cleaned_dataset.csv', index=False)

Exercise 2: Data Visualization
Objective: Learn how to visualize data to gain insights using Python and matplotlib.

pip install pandas matplotlib seaborn

Import Libraries:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:
df = pd.read_csv('cleaned_dataset.csv')

Plot Histograms:
df.hist(figsize=(10, 8))
plt.show()

Create a Correlation Matrix:
corr_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

Plot Scatter Plots:
sns.pairplot(df)
plt.show()


Building a Simple Recommender System
A recommender system suggests items to users based on their preferences and behavior.

pip install numpy pandas scikit-learn

Import Libraries:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error

Load the Dataset:
df = pd.read_csv('path_to_movielens_dataset.csv')

Preprocess the Data:
# Pivot the data to create a user-item matrix
user_movie_matrix = df.pivot(index='userId', columns='movieId', values='rating')
# Fill NaN values with 0
user_movie_matrix.fillna(0, inplace=True)

Compute Similarity Matrix:
similarity_matrix = cosine_similarity(user_movie_matrix)

Build the Recommender System:
def recommend_movies(user_id, num_recommendations):
    user_index = user_id - 1
    similar_users = similarity_matrix[user_index]
    # Get the top similar users
    similar_users_index = similar_users.argsort()[::-1][1:num_recommendations+1]
    # Get the movies these users liked
    recommended_movies = user_movie_matrix.iloc[similar_users_index].mean(axis=0).sort_values(ascending=False)
    return recommended_movies.head(num_recommendations)

# Recommend movies for user with ID 1
recommendations = recommend_movies(1, 5)
print(recommendations)

Creating a Basic Image Classifier
An image classifier identifies the category of an object in an image.

pip install tensorflow keras numpy matplotlib

Import Libraries:
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:
plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()


Analyzing Sentiments Using NLP
Sentiment analysis determines the sentiment expressed in a piece of text (positive, negative, neutral). 
pip install nltk

Import Libraries:
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

Download NLTK Data:
nltk.download('vader_lexicon')

Initialize the Sentiment Analyzer:
sia = SentimentIntensityAnalyzer()

Analyze Sentiment of a Sentence:
sentence = "NLTK is a great library for NLP!"
sentiment = sia.polarity_scores(sentence)
print(sentiment)

Practical Exercises
Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models.

pip install pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:

For this example, weâ€™ll use a hypothetical dataset that includes demographic information and outcomes (e.g., loan approval data):

df = pd.read_csv('loan_approval_data.csv')

Inspect the Dataset:
print(df.head())
print(df.info())

Analyze Demographic Distribution:
gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyze Outcome Distribution:

Check the distribution of outcomes (e.g., loan approvals) across different demographic groups:
approval_by_gender = df.groupby('gender')['loan_approved'].mean()
approval_by_race = df.groupby('race')['loan_approved'].mean()

print(approval_by_gender)
print(approval_by_race)

sns.barplot(x='gender', y='loan_approved', data=df)
plt.show()

sns.barplot(x='race', y='loan_approved', data=df)
plt.show()

Calculate Bias Metrics:

Use fairness metrics to measure bias in the dataset:
python

from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')

Creating Fair AI Models
Objective: Develop AI models that mitigate bias and ensure fair outcomes across different demographic groups.

pip install pandas numpy scikit-learn aif360

Import Libraries:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing

Load and Preprocess the Dataset:
df = pd.read_csv('loan_approval_data.csv')
df['gender'] = df['gender'].map({'male': 1, 'female': 0})
df['race'] = df['race'].map({'white': 1, 'non_white': 0})

X = df.drop('loan_approved', axis=1)
y = df['loan_approved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

Train a Baseline Model:
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

Assess Bias in the Model:
test_dataset = BinaryLabelDataset(df=pd.concat([X_test, y_test], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
metric = BinaryLabelDatasetMetric(test_dataset, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact (Gender): {metric.disparate_impact()}')
print(f'Disparate Impact (Race): {metric.disparate_impact()}')

Mitigate Bias Using Reweighing:
RW = Reweighing(unprivileged_groups=[{'gender': 0}, {'race': 0}], privileged_groups=[{'gender': 1}, {'race': 1}])
train_dataset = BinaryLabelDataset(df=pd.concat([X_train, y_train], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
train_dataset_transf = RW.fit_transform(train_dataset)

model.fit(train_dataset_transf.features, train_dataset_transf.labels.ravel())
y_pred_transf = model.predict(X_test)

print(f'Accuracy after Reweighing: {accuracy_score(y_test, y_pred_transf)}')
print(confusion_matrix(y_test, y_pred_transf))

test_dataset_pred_transf = test_dataset.copy(deepcopy=True)
test_dataset_pred_transf.labels = y_pred_transf

metric_transf = BinaryLabelDatasetMetric(test_dataset_pred_transf, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact after Reweighing (Gender): {metric_transf.disparate_impact()}')
print(f'Disparate Impact after Reweighing (Race): {metric_transf.disparate_impact()}')


Unit 3  Skill Improvement: Practical Implementation with NumPy
Installation:
pip install numpy

Basic NumPy Operations:
import numpy as np

# Define matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Addition
C = A + B
print("Addition:\n", C)

# Subtraction
D = A - B
print("Subtraction:\n", D)

# Multiplication
E = np.dot(A, B)
print("Multiplication:\n", E)

# Transpose
A_T = A.T
print("Transpose:\n", A_T)

Example Applications:

Data Representation:

# Representing a dataset with 3 samples and 2 features
data = np.array([[1, 2], [3, 4], [5, 6]])
print("Dataset:\n", data)

Linear Transformation:
# Applying a linear transformation
transformation_matrix = np.array([[0, 1], [-1, 0]])  # 90-degree rotation matrix
transformed_data = np.dot(data, transformation_matrix)
print("Transformed Data:\n", transformed_data)

Image Processing:
# Assuming image is represented as a 2D matrix of pixel values
image = np.array([[100, 150], [200, 250]])
filter = np.array([[0, 1], [-1, 0]])  # Example filter
processed_image = np.dot(image, filter)
print("Processed Image:\n", processed_image)

Skill Improvement: Set Operations Using Python Libraries
Using Python's Built-in Set Functions
Basic Set Operations:
# Define sets
A = {1, 2, 3}
B = {3, 4, 5}

# Union
union_set = A | B
print("Union:", union_set)

# Intersection
intersection_set = A & B
print("Intersection:", intersection_set)

# Complement (difference in Python)
complement_set = A - B
print("Complement of A relative to B:", complement_set)

Example Applications in Data Cleaning:
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4]})
df2 = pd.DataFrame({'customer_id': [3, 4, 5, 6]})

# Convert columns to sets
set1 = set(df1['customer_id'])
set2 = set(df2['customer_id'])

# Find common customers (intersection)
common_customers = set1 & set2
print("Common customers:", common_customers)

# Find unique customers in df1 (difference)
unique_to_df1 = set1 - set2
print("Unique to df1:", unique_to_df1)

Handling Missing Values:
# Sample DataFrame with missing values
df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]})

# Find rows with missing values in any column
missing_indices = set(df[df.isnull().any(axis=1)].index)
print("Rows with missing values:", missing_indices)

Merging Datasets:
# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})
df2 = pd.DataFrame({'customer_id': [3, 4, 5], 'name': ['Charlie', 'David', 'Eve']})

# Merge DataFrames using set intersection
merged_df = pd.merge(df1, df2, on='customer_id', how='inner')
print("Merged DataFrame on common customers:\n", merged_df)

Skill Improvement: Using Statistical Libraries in Python
Using SciPy and Statsmodels for Calculations and Visualizations
SciPy:
SciPy is a Python library used for scientific and technical computing. It provides modules for statistics, optimization, integration, and more.

Statsmodels:
Statsmodels is a Python library that provides classes and functions for the estimation of statistical models, performing statistical tests, and data exploration.

Example Code:

Calculating Measures of Central Tendency:
import numpy as np
from scipy import stats

data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]

# Mean
mean = np.mean(data)
print("Mean:", mean)

# Median
median = np.median(data)
print("Median:", median)

# Mode
mode = stats.mode(data)
print("Mode:", mode.mode[0])

Calculating Measures of Dispersion:
# Range
data_range = np.ptp(data)
print("Range:", data_range)

# Variance
variance = np.var(data)
print("Variance:", variance)

# Standard Deviation
std_dev = np.std(data)
print("Standard Deviation:", std_dev)

Probability Calculations:
# Basic Probability Example
from fractions import Fraction

favorable_outcomes = 1  # e.g., probability of rolling a 6 on a die
total_outcomes = 6
probability = Fraction(favorable_outcomes, total_outcomes)
print("Probability of rolling a 6:", float(probability))

Visualizing Data:
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.hist(data, bins=5, edgecolor='black')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

# Box Plot
sns.boxplot(data)
plt.title('Box Plot')
plt.show()

# Scatter Plot
x = np.arange(len(data))
y = data
plt.scatter(x, y)
plt.title('Scatter Plot')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

Visualization Techniques
Bar Graphs:

Used to display categorical data with rectangular bars.
The length of each bar represents the value of the category.
Effective for comparing different groups or categories.
Example:

import matplotlib.pyplot as plt

categories = ['A', 'B', 'C']
values = [10, 20, 15]

plt.bar(categories, values, color='skyblue')
plt.title('Bar Graph')
plt.xlabel('Category')
plt.ylabel('Value')
plt.show()

Histograms:
data = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]

plt.hist(data, bins=5, edgecolor='black', color='lightgreen')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

Scatter Plots:
x = [1, 2, 3, 4, 5]
y = [5, 7, 8, 5, 6]

plt.scatter(x, y, color='red')
plt.title('Scatter Plot')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

Matplotlib Example:
import matplotlib.pyplot as plt

# Line Plot
x = [1, 2, 3, 4, 5]
y = [2, 3, 5, 7, 11]
plt.plot(x, y, marker='o')
plt.title('Line Plot')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

Seaborn Example:
import seaborn as sns
import pandas as pd

# Sample DataFrame
data = pd.DataFrame({
    'x': [1, 2, 3, 4, 5],
    'y': [2, 3, 5, 7, 11]
})

# Seaborn Line Plot
sns.lineplot(x='x', y='y', data=data)
plt.title('Seaborn Line Plot')
plt.show()

Bar Graph Exercise:
categories = ['Electronics', 'Clothing', 'Home']
sales = [120, 80, 150]

plt.bar(categories, sales, color='orange')
plt.title('Product Sales by Category')
plt.xlabel('Category')
plt.ylabel('Number of Products Sold')
plt.show()

Histogram Exercise:
scores = [55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

plt.hist(scores, bins=5, edgecolor='black', color='purple')
plt.title('Distribution of Test Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()

Scatter Plot Exercise:
advertising_spend = [500, 600, 700, 800, 900]
sales_revenue = [2000, 2200, 2400, 2600, 2800]

plt.scatter(advertising_spend, sales_revenue, color='blue')
plt.title('Advertising Spend vs. Sales Revenue')
plt.xlabel('Advertising Spend ($)')
plt.ylabel('Sales Revenue ($)')
plt.show()

Demonstrate PCA Using Python Libraries (Scikit-learn)
Installing Scikit-learn:
pip install scikit-learn

PCA Implementation:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the data
from sklearn.preprocessing import StandardScaler
X_standardized = StandardScaler().fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
principal_components = pca.fit_transform(X_standardized)

# Plot the results
plt.figure(figsize=(8, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.colorbar()
plt.show()

Skill Improvement: Implementing Linear Regression Using Python Libraries

pip install scikit-learn

Linear Regression Implementation:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable
y = np.array([2, 3, 5, 6, 8])  # Dependent variable

# Create and fit the model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Model parameters
slope = model.coef_[0]
intercept = model.intercept_

print("Slope:", slope)
print("Intercept:", intercept)

# Evaluation metrics
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Plotting the results
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X, y_pred, color='red', label='Fitted line')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

Analyzing Bias in Data:
import pandas as pd

# Load dataset
data = pd.read_csv('example_dataset.csv')

# Check for representation
print(data['demographic_column'].value_counts())

# Visualize potential bias
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='demographic_column', hue='target_column', data=data)
plt.title('Bias Analysis')
plt.show()

Implementing Fairness Constraints:
from sklearn.linear_model import LogisticRegression
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

# Load dataset
X = data.drop(columns='target_column')
y = data['target_column']

# Train model with fairness constraint
estimator = LogisticRegression()
constraint = DemographicParity()
mitigator = ExponentiatedGradient(estimator, constraints=constraint)
mitigator.fit(X, y, sensitive_features=data['demographic_column'])

# Evaluate model
predictions = mitigator.predict(X)
print("Fairness Metrics:", mitigator.fairness_metric())


Skill Improvement: Set Operations Using Python Libraries
Using Python's Built-in Set Functions
Basic Set Operations:
# Define sets
A = {1, 2, 3}
B = {3, 4, 5}

# Union
union_set = A | B
print("Union:", union_set)

# Intersection
intersection_set = A & B
print("Intersection:", intersection_set)

# Complement (difference in Python)
complement_set = A - B
print("Complement of A relative to B:", complement_set)

Example Applications in Data Cleaning:
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4]})
df2 = pd.DataFrame({'customer_id': [3, 4, 5, 6]})

# Convert columns to sets
set1 = set(df1['customer_id'])
set2 = set(df2['customer_id'])

# Find common customers (intersection)
common_customers = set1 & set2
print("Common customers:", common_customers)

# Find unique customers in df1 (difference)
unique_to_df1 = set1 - set2
print("Unique to df1:", unique_to_df1)

Handling Missing Values:
# Sample DataFrame with missing values
df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]})

# Find rows with missing values in any column
missing_indices = set(df[df.isnull().any(axis=1)].index)
print("Rows with missing values:", missing_indices)

Merging Datasets:
# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})
df2 = pd.DataFrame({'customer_id': [3, 4, 5], 'name': ['Charlie', 'David', 'Eve']})

# Merge DataFrames using set intersection
merged_df = pd.merge(df1, df2, on='customer_id', how='inner')
print("Merged DataFrame on common customers:\n", merged_df)

Skill Improvement: Using Statistical Libraries in Python
Using SciPy and Statsmodels for Calculations and Visualizations
SciPy:
SciPy is a Python library used for scientific and technical computing. It provides modules for statistics, optimization, integration, and more.

Example Code:

Calculating Measures of Central Tendency:
import numpy as np
from scipy import stats

data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]

# Mean
mean = np.mean(data)
print("Mean:", mean)

# Median
median = np.median(data)
print("Median:", median)

# Mode
mode = stats.mode(data)
print("Mode:", mode.mode[0])

Calculating Measures of Dispersion:
# Range
data_range = np.ptp(data)
print("Range:", data_range)

# Variance
variance = np.var(data)
print("Variance:", variance)

# Standard Deviation
std_dev = np.std(data)
print("Standard Deviation:", std_dev)

Probability Calculations:
# Basic Probability Example
from fractions import Fraction

favorable_outcomes = 1  # e.g., probability of rolling a 6 on a die
total_outcomes = 6
probability = Fraction(favorable_outcomes, total_outcomes)
print("Probability of rolling a 6:", float(probability))

Visualizing Data:
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.hist(data, bins=5, edgecolor='black')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

# Box Plot
sns.boxplot(data)
plt.title('Box Plot')
plt.show()

# Scatter Plot
x = np.arange(len(data))
y = data
plt.scatter(x, y)
plt.title('Scatter Plot')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models

pip install pandas numpy matplotlib seaborn

Import Libraries:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:
df = pd.read_csv('loan_approval_data.csv')

Inspect the Dataset:
print(df.head())
print(df.info())

Analyze Demographic Distribution:
gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyze Outcome Distribution:
approval_by_gender = df.groupby('gender')['loan_approved'].mean()
approval_by_race = df.groupby('race')['loan_approved'].mean()

print(approval_by_gender)
print(approval_by_race)

sns.barplot(x='gender', y='loan_approved', data=df)
plt.show()

sns.barplot(x='race', y='loan_approved', data=df)
plt.show()

Calculate Bias Metrics:
from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')




