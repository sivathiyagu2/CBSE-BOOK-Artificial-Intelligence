Unit 1 INTRODUCTION TO AI

Project 1: Creating a Basic Chatbot

A chatbot is an AI application designed to simulate conversation with human users. This project will guide you through creating a basic rule-based chatbot using Python.
Step-by-Step Guide:
Set Up the Environment:
Install Python and a text editor or IDE (e.g., Visual Studio Code).


pip install nltk
import nltk
from nltk.chat.util import Chat, reflections

pairs = [
    [
        r"my name is (.*)",
        ["Hello %1, How can I help you today?",]
    ],
    [
        r"hi|hello|hey",
        ["Hello!", "Hey there!",]
    ],
    [
        r"what is your name?",
        ["I am a chatbot created to assist you.",]
    ],
    [
        r"how are you?",
        ["I'm doing good. How about you?",]
    ],
    [
        r"sorry (.*)",
        ["It's okay.", "No problem at all.",]
    ],
    [
        r"quit",
        ["Bye! Take care.",]
    ],
]


def chatbot():
    print("Hi! I am your chatbot. Type 'quit' to exit.")
    chat = Chat(pairs, reflections)
    chat.converse()

if __name__ == "__main__":
    chatbot()

python chatbot.py


Project 2: Image Recognition Using Pretrained Models
Image recognition involves identifying objects or features within an image. This project uses a pretrained model to classify images.

Step-by-Step Guide:

pip install tensorflow keras numpy matplotlib

Import Libraries:

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'  # Replace with the path to your image
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:

plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()

Run the Script:
python image_recognition.py


Exercise 1: Data Cleaning
Objective: Learn how to clean and preprocess a dataset using Python and pandas.

pip install pandas numpy

Import Libraries:
import pandas as pd
import numpy as np

Load the Dataset:
df = pd.read_csv('path_to_your_dataset.csv')  # Replace with the path to your dataset

Inspect the Dataset:
print(df.head())
print(df.info())

Handle Missing Values

Identify Missing Values:
print(df.isnull().sum())

Fill Missing Values:
df.fillna(df.mean(), inplace=True)

Drop Rows with Missing Values:
df.dropna(inplace=True)

Normalize Data:
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
df_scaled = pd.DataFrame(df_scaled, columns=df.columns)

Save the Cleaned Dataset:
df_scaled.to_csv('cleaned_dataset.csv', index=False)

Exercise 2: Data Visualization
Objective: Learn how to visualize data to gain insights using Python and matplotlib.

pip install pandas matplotlib seaborn

Import Libraries:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:
df = pd.read_csv('cleaned_dataset.csv')

Plot Histograms:
df.hist(figsize=(10, 8))
plt.show()

Create a Correlation Matrix:
corr_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

Plot Scatter Plots:
sns.pairplot(df)
plt.show()


Building a Simple Recommender System
A recommender system suggests items to users based on their preferences and behavior.

pip install numpy pandas scikit-learn

Import Libraries:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error

Load the Dataset:
df = pd.read_csv('path_to_movielens_dataset.csv')

Preprocess the Data:
# Pivot the data to create a user-item matrix
user_movie_matrix = df.pivot(index='userId', columns='movieId', values='rating')
# Fill NaN values with 0
user_movie_matrix.fillna(0, inplace=True)

Compute Similarity Matrix:
similarity_matrix = cosine_similarity(user_movie_matrix)

Build the Recommender System:
def recommend_movies(user_id, num_recommendations):
    user_index = user_id - 1
    similar_users = similarity_matrix[user_index]
    # Get the top similar users
    similar_users_index = similar_users.argsort()[::-1][1:num_recommendations+1]
    # Get the movies these users liked
    recommended_movies = user_movie_matrix.iloc[similar_users_index].mean(axis=0).sort_values(ascending=False)
    return recommended_movies.head(num_recommendations)

# Recommend movies for user with ID 1
recommendations = recommend_movies(1, 5)
print(recommendations)

Creating a Basic Image Classifier
An image classifier identifies the category of an object in an image.

pip install tensorflow keras numpy matplotlib

Import Libraries:
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:
plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()


Analyzing Sentiments Using NLP
Sentiment analysis determines the sentiment expressed in a piece of text (positive, negative, neutral). 
pip install nltk

Import Libraries:
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

Download NLTK Data:
nltk.download('vader_lexicon')

Initialize the Sentiment Analyzer:
sia = SentimentIntensityAnalyzer()

Analyze Sentiment of a Sentence:
sentence = "NLTK is a great library for NLP!"
sentiment = sia.polarity_scores(sentence)
print(sentiment)

Practical Exercises
Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models.

pip install pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:

For this example, weâ€™ll use a hypothetical dataset that includes demographic information and outcomes (e.g., loan approval data):

df = pd.read_csv('loan_approval_data.csv')

Inspect the Dataset:
print(df.head())
print(df.info())

Analyze Demographic Distribution:
gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyze Outcome Distribution:

Check the distribution of outcomes (e.g., loan approvals) across different demographic groups:
approval_by_gender = df.groupby('gender')['loan_approved'].mean()
approval_by_race = df.groupby('race')['loan_approved'].mean()

print(approval_by_gender)
print(approval_by_race)

sns.barplot(x='gender', y='loan_approved', data=df)
plt.show()

sns.barplot(x='race', y='loan_approved', data=df)
plt.show()

Calculate Bias Metrics:

Use fairness metrics to measure bias in the dataset:
python

from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')

Creating Fair AI Models
Objective: Develop AI models that mitigate bias and ensure fair outcomes across different demographic groups.

pip install pandas numpy scikit-learn aif360

Import Libraries:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing

Load and Preprocess the Dataset:
df = pd.read_csv('loan_approval_data.csv')
df['gender'] = df['gender'].map({'male': 1, 'female': 0})
df['race'] = df['race'].map({'white': 1, 'non_white': 0})

X = df.drop('loan_approved', axis=1)
y = df['loan_approved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

Train a Baseline Model:
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

Assess Bias in the Model:
test_dataset = BinaryLabelDataset(df=pd.concat([X_test, y_test], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
metric = BinaryLabelDatasetMetric(test_dataset, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact (Gender): {metric.disparate_impact()}')
print(f'Disparate Impact (Race): {metric.disparate_impact()}')

Mitigate Bias Using Reweighing:
RW = Reweighing(unprivileged_groups=[{'gender': 0}, {'race': 0}], privileged_groups=[{'gender': 1}, {'race': 1}])
train_dataset = BinaryLabelDataset(df=pd.concat([X_train, y_train], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
train_dataset_transf = RW.fit_transform(train_dataset)

model.fit(train_dataset_transf.features, train_dataset_transf.labels.ravel())
y_pred_transf = model.predict(X_test)

print(f'Accuracy after Reweighing: {accuracy_score(y_test, y_pred_transf)}')
print(confusion_matrix(y_test, y_pred_transf))

test_dataset_pred_transf = test_dataset.copy(deepcopy=True)
test_dataset_pred_transf.labels = y_pred_transf

metric_transf = BinaryLabelDatasetMetric(test_dataset_pred_transf, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact after Reweighing (Gender): {metric_transf.disparate_impact()}')
print(f'Disparate Impact after Reweighing (Race): {metric_transf.disparate_impact()}')


Unit 3  Skill Improvement: Practical Implementation with NumPy
Installation:
pip install numpy

Basic NumPy Operations:
import numpy as np

# Define matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Addition
C = A + B
print("Addition:\n", C)

# Subtraction
D = A - B
print("Subtraction:\n", D)

# Multiplication
E = np.dot(A, B)
print("Multiplication:\n", E)

# Transpose
A_T = A.T
print("Transpose:\n", A_T)

Example Applications:

Data Representation:

# Representing a dataset with 3 samples and 2 features
data = np.array([[1, 2], [3, 4], [5, 6]])
print("Dataset:\n", data)

Linear Transformation:
# Applying a linear transformation
transformation_matrix = np.array([[0, 1], [-1, 0]])  # 90-degree rotation matrix
transformed_data = np.dot(data, transformation_matrix)
print("Transformed Data:\n", transformed_data)

Image Processing:
# Assuming image is represented as a 2D matrix of pixel values
image = np.array([[100, 150], [200, 250]])
filter = np.array([[0, 1], [-1, 0]])  # Example filter
processed_image = np.dot(image, filter)
print("Processed Image:\n", processed_image)


