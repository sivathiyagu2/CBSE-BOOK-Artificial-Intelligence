Unit 1 INTRODUCTION TO AI

Project 1: Creating a Basic Chatbot

A chatbot is an AI application designed to simulate conversation with human users. This project will guide you through creating a basic rule-based chatbot using Python.
Step-by-Step Guide:
Set Up the Environment:
Install Python and a text editor or IDE (e.g., Visual Studio Code).


pip install nltk
import nltk
from nltk.chat.util import Chat, reflections

pairs = [
    [
        r"my name is (.*)",
        ["Hello %1, How can I help you today?",]
    ],
    [
        r"hi|hello|hey",
        ["Hello!", "Hey there!",]
    ],
    [
        r"what is your name?",
        ["I am a chatbot created to assist you.",]
    ],
    [
        r"how are you?",
        ["I'm doing good. How about you?",]
    ],
    [
        r"sorry (.*)",
        ["It's okay.", "No problem at all.",]
    ],
    [
        r"quit",
        ["Bye! Take care.",]
    ],
]


def chatbot():
    print("Hi! I am your chatbot. Type 'quit' to exit.")
    chat = Chat(pairs, reflections)
    chat.converse()

if __name__ == "__main__":
    chatbot()

python chatbot.py


Project 2: Image Recognition Using Pretrained Models
Image recognition involves identifying objects or features within an image. This project uses a pretrained model to classify images.

Step-by-Step Guide:

pip install tensorflow keras numpy matplotlib

Import Libraries:

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'  # Replace with the path to your image
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:

plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()

Run the Script:
python image_recognition.py


Exercise 1: Data Cleaning
Objective: Learn how to clean and preprocess a dataset using Python and pandas.

pip install pandas numpy

Import Libraries:
import pandas as pd
import numpy as np

Load the Dataset:
df = pd.read_csv('path_to_your_dataset.csv')  # Replace with the path to your dataset

Inspect the Dataset:
print(df.head())
print(df.info())

Handle Missing Values

Identify Missing Values:
print(df.isnull().sum())

Fill Missing Values:
df.fillna(df.mean(), inplace=True)

Drop Rows with Missing Values:
df.dropna(inplace=True)

Normalize Data:
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
df_scaled = pd.DataFrame(df_scaled, columns=df.columns)

Save the Cleaned Dataset:
df_scaled.to_csv('cleaned_dataset.csv', index=False)

Exercise 2: Data Visualization
Objective: Learn how to visualize data to gain insights using Python and matplotlib.

pip install pandas matplotlib seaborn

Import Libraries:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:
df = pd.read_csv('cleaned_dataset.csv')

Plot Histograms:
df.hist(figsize=(10, 8))
plt.show()

Create a Correlation Matrix:
corr_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

Plot Scatter Plots:
sns.pairplot(df)
plt.show()


Building a Simple Recommender System
A recommender system suggests items to users based on their preferences and behavior.

pip install numpy pandas scikit-learn

Import Libraries:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error

Load the Dataset:
df = pd.read_csv('path_to_movielens_dataset.csv')

Preprocess the Data:
# Pivot the data to create a user-item matrix
user_movie_matrix = df.pivot(index='userId', columns='movieId', values='rating')
# Fill NaN values with 0
user_movie_matrix.fillna(0, inplace=True)

Compute Similarity Matrix:
similarity_matrix = cosine_similarity(user_movie_matrix)

Build the Recommender System:
def recommend_movies(user_id, num_recommendations):
    user_index = user_id - 1
    similar_users = similarity_matrix[user_index]
    # Get the top similar users
    similar_users_index = similar_users.argsort()[::-1][1:num_recommendations+1]
    # Get the movies these users liked
    recommended_movies = user_movie_matrix.iloc[similar_users_index].mean(axis=0).sort_values(ascending=False)
    return recommended_movies.head(num_recommendations)

# Recommend movies for user with ID 1
recommendations = recommend_movies(1, 5)
print(recommendations)

Creating a Basic Image Classifier
An image classifier identifies the category of an object in an image.

pip install tensorflow keras numpy matplotlib

Import Libraries:
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

Load the Pretrained Model:
model = ResNet50(weights='imagenet')

Load and Preprocess the Image:
img_path = 'path_to_your_image.jpg'
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

Make Predictions:
predictions = model.predict(img_array)
decoded_predictions = decode_predictions(predictions, top=3)[0]

for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i+1}: {label} ({score:.2f})")

Display the Image and Predictions:
plt.imshow(img)
plt.axis('off')
plt.title("Predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    plt.text(0, 250 + i*20, f"{i+1}: {label} ({score:.2f})", color='red')
plt.show()


Analyzing Sentiments Using NLP
Sentiment analysis determines the sentiment expressed in a piece of text (positive, negative, neutral). 
pip install nltk

Import Libraries:
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

Download NLTK Data:
nltk.download('vader_lexicon')

Initialize the Sentiment Analyzer:
sia = SentimentIntensityAnalyzer()

Analyze Sentiment of a Sentence:
sentence = "NLTK is a great library for NLP!"
sentiment = sia.polarity_scores(sentence)
print(sentiment)

Practical Exercises
Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models.

pip install pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:

For this example, we’ll use a hypothetical dataset that includes demographic information and outcomes (e.g., loan approval data):

df = pd.read_csv('loan_approval_data.csv')

Inspect the Dataset:
print(df.head())
print(df.info())

Analyze Demographic Distribution:
gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyze Outcome Distribution:

Check the distribution of outcomes (e.g., loan approvals) across different demographic groups:
approval_by_gender = df.groupby('gender')['loan_approved'].mean()
approval_by_race = df.groupby('race')['loan_approved'].mean()

print(approval_by_gender)
print(approval_by_race)

sns.barplot(x='gender', y='loan_approved', data=df)
plt.show()

sns.barplot(x='race', y='loan_approved', data=df)
plt.show()

Calculate Bias Metrics:

Use fairness metrics to measure bias in the dataset:
python

from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')

Creating Fair AI Models
Objective: Develop AI models that mitigate bias and ensure fair outcomes across different demographic groups.

pip install pandas numpy scikit-learn aif360

Import Libraries:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing

Load and Preprocess the Dataset:
df = pd.read_csv('loan_approval_data.csv')
df['gender'] = df['gender'].map({'male': 1, 'female': 0})
df['race'] = df['race'].map({'white': 1, 'non_white': 0})

X = df.drop('loan_approved', axis=1)
y = df['loan_approved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

Train a Baseline Model:
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

Assess Bias in the Model:
test_dataset = BinaryLabelDataset(df=pd.concat([X_test, y_test], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
metric = BinaryLabelDatasetMetric(test_dataset, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact (Gender): {metric.disparate_impact()}')
print(f'Disparate Impact (Race): {metric.disparate_impact()}')

Mitigate Bias Using Reweighing:
RW = Reweighing(unprivileged_groups=[{'gender': 0}, {'race': 0}], privileged_groups=[{'gender': 1}, {'race': 1}])
train_dataset = BinaryLabelDataset(df=pd.concat([X_train, y_train], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
train_dataset_transf = RW.fit_transform(train_dataset)

model.fit(train_dataset_transf.features, train_dataset_transf.labels.ravel())
y_pred_transf = model.predict(X_test)

print(f'Accuracy after Reweighing: {accuracy_score(y_test, y_pred_transf)}')
print(confusion_matrix(y_test, y_pred_transf))

test_dataset_pred_transf = test_dataset.copy(deepcopy=True)
test_dataset_pred_transf.labels = y_pred_transf

metric_transf = BinaryLabelDatasetMetric(test_dataset_pred_transf, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact after Reweighing (Gender): {metric_transf.disparate_impact()}')
print(f'Disparate Impact after Reweighing (Race): {metric_transf.disparate_impact()}')


Unit 3  Skill Improvement: Practical Implementation with NumPy
Installation:
pip install numpy

Basic NumPy Operations:
import numpy as np

# Define matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Addition
C = A + B
print("Addition:\n", C)

# Subtraction
D = A - B
print("Subtraction:\n", D)

# Multiplication
E = np.dot(A, B)
print("Multiplication:\n", E)

# Transpose
A_T = A.T
print("Transpose:\n", A_T)

Example Applications:

Data Representation:

# Representing a dataset with 3 samples and 2 features
data = np.array([[1, 2], [3, 4], [5, 6]])
print("Dataset:\n", data)

Linear Transformation:
# Applying a linear transformation
transformation_matrix = np.array([[0, 1], [-1, 0]])  # 90-degree rotation matrix
transformed_data = np.dot(data, transformation_matrix)
print("Transformed Data:\n", transformed_data)

Image Processing:
# Assuming image is represented as a 2D matrix of pixel values
image = np.array([[100, 150], [200, 250]])
filter = np.array([[0, 1], [-1, 0]])  # Example filter
processed_image = np.dot(image, filter)
print("Processed Image:\n", processed_image)

Skill Improvement: Set Operations Using Python Libraries
Using Python's Built-in Set Functions
Basic Set Operations:
# Define sets
A = {1, 2, 3}
B = {3, 4, 5}

# Union
union_set = A | B
print("Union:", union_set)

# Intersection
intersection_set = A & B
print("Intersection:", intersection_set)

# Complement (difference in Python)
complement_set = A - B
print("Complement of A relative to B:", complement_set)

Example Applications in Data Cleaning:
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4]})
df2 = pd.DataFrame({'customer_id': [3, 4, 5, 6]})

# Convert columns to sets
set1 = set(df1['customer_id'])
set2 = set(df2['customer_id'])

# Find common customers (intersection)
common_customers = set1 & set2
print("Common customers:", common_customers)

# Find unique customers in df1 (difference)
unique_to_df1 = set1 - set2
print("Unique to df1:", unique_to_df1)

Handling Missing Values:
# Sample DataFrame with missing values
df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]})

# Find rows with missing values in any column
missing_indices = set(df[df.isnull().any(axis=1)].index)
print("Rows with missing values:", missing_indices)

Merging Datasets:
# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})
df2 = pd.DataFrame({'customer_id': [3, 4, 5], 'name': ['Charlie', 'David', 'Eve']})

# Merge DataFrames using set intersection
merged_df = pd.merge(df1, df2, on='customer_id', how='inner')
print("Merged DataFrame on common customers:\n", merged_df)

Skill Improvement: Using Statistical Libraries in Python
Using SciPy and Statsmodels for Calculations and Visualizations
SciPy:
SciPy is a Python library used for scientific and technical computing. It provides modules for statistics, optimization, integration, and more.

Statsmodels:
Statsmodels is a Python library that provides classes and functions for the estimation of statistical models, performing statistical tests, and data exploration.

Example Code:

Calculating Measures of Central Tendency:
import numpy as np
from scipy import stats

data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]

# Mean
mean = np.mean(data)
print("Mean:", mean)

# Median
median = np.median(data)
print("Median:", median)

# Mode
mode = stats.mode(data)
print("Mode:", mode.mode[0])

Calculating Measures of Dispersion:
# Range
data_range = np.ptp(data)
print("Range:", data_range)

# Variance
variance = np.var(data)
print("Variance:", variance)

# Standard Deviation
std_dev = np.std(data)
print("Standard Deviation:", std_dev)

Probability Calculations:
# Basic Probability Example
from fractions import Fraction

favorable_outcomes = 1  # e.g., probability of rolling a 6 on a die
total_outcomes = 6
probability = Fraction(favorable_outcomes, total_outcomes)
print("Probability of rolling a 6:", float(probability))

Visualizing Data:
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.hist(data, bins=5, edgecolor='black')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

# Box Plot
sns.boxplot(data)
plt.title('Box Plot')
plt.show()

# Scatter Plot
x = np.arange(len(data))
y = data
plt.scatter(x, y)
plt.title('Scatter Plot')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

Visualization Techniques
Bar Graphs:

Used to display categorical data with rectangular bars.
The length of each bar represents the value of the category.
Effective for comparing different groups or categories.
Example:

import matplotlib.pyplot as plt

categories = ['A', 'B', 'C']
values = [10, 20, 15]

plt.bar(categories, values, color='skyblue')
plt.title('Bar Graph')
plt.xlabel('Category')
plt.ylabel('Value')
plt.show()

Histograms:
data = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]

plt.hist(data, bins=5, edgecolor='black', color='lightgreen')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

Scatter Plots:
x = [1, 2, 3, 4, 5]
y = [5, 7, 8, 5, 6]

plt.scatter(x, y, color='red')
plt.title('Scatter Plot')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

Matplotlib Example:
import matplotlib.pyplot as plt

# Line Plot
x = [1, 2, 3, 4, 5]
y = [2, 3, 5, 7, 11]
plt.plot(x, y, marker='o')
plt.title('Line Plot')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

Seaborn Example:
import seaborn as sns
import pandas as pd

# Sample DataFrame
data = pd.DataFrame({
    'x': [1, 2, 3, 4, 5],
    'y': [2, 3, 5, 7, 11]
})

# Seaborn Line Plot
sns.lineplot(x='x', y='y', data=data)
plt.title('Seaborn Line Plot')
plt.show()

Bar Graph Exercise:
categories = ['Electronics', 'Clothing', 'Home']
sales = [120, 80, 150]

plt.bar(categories, sales, color='orange')
plt.title('Product Sales by Category')
plt.xlabel('Category')
plt.ylabel('Number of Products Sold')
plt.show()

Histogram Exercise:
scores = [55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

plt.hist(scores, bins=5, edgecolor='black', color='purple')
plt.title('Distribution of Test Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()

Scatter Plot Exercise:
advertising_spend = [500, 600, 700, 800, 900]
sales_revenue = [2000, 2200, 2400, 2600, 2800]

plt.scatter(advertising_spend, sales_revenue, color='blue')
plt.title('Advertising Spend vs. Sales Revenue')
plt.xlabel('Advertising Spend ($)')
plt.ylabel('Sales Revenue ($)')
plt.show()

Demonstrate PCA Using Python Libraries (Scikit-learn)
Installing Scikit-learn:
pip install scikit-learn

PCA Implementation:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the data
from sklearn.preprocessing import StandardScaler
X_standardized = StandardScaler().fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
principal_components = pca.fit_transform(X_standardized)

# Plot the results
plt.figure(figsize=(8, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.colorbar()
plt.show()

Skill Improvement: Implementing Linear Regression Using Python Libraries

pip install scikit-learn

Linear Regression Implementation:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable
y = np.array([2, 3, 5, 6, 8])  # Dependent variable

# Create and fit the model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Model parameters
slope = model.coef_[0]
intercept = model.intercept_

print("Slope:", slope)
print("Intercept:", intercept)

# Evaluation metrics
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Plotting the results
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X, y_pred, color='red', label='Fitted line')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

Analyzing Bias in Data:
import pandas as pd

# Load dataset
data = pd.read_csv('example_dataset.csv')

# Check for representation
print(data['demographic_column'].value_counts())

# Visualize potential bias
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='demographic_column', hue='target_column', data=data)
plt.title('Bias Analysis')
plt.show()

Implementing Fairness Constraints:
from sklearn.linear_model import LogisticRegression
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

# Load dataset
X = data.drop(columns='target_column')
y = data['target_column']

# Train model with fairness constraint
estimator = LogisticRegression()
constraint = DemographicParity()
mitigator = ExponentiatedGradient(estimator, constraints=constraint)
mitigator.fit(X, y, sensitive_features=data['demographic_column'])

# Evaluate model
predictions = mitigator.predict(X)
print("Fairness Metrics:", mitigator.fairness_metric())


Skill Improvement: Set Operations Using Python Libraries
Using Python's Built-in Set Functions
Basic Set Operations:
# Define sets
A = {1, 2, 3}
B = {3, 4, 5}

# Union
union_set = A | B
print("Union:", union_set)

# Intersection
intersection_set = A & B
print("Intersection:", intersection_set)

# Complement (difference in Python)
complement_set = A - B
print("Complement of A relative to B:", complement_set)

Example Applications in Data Cleaning:
import pandas as pd

# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4]})
df2 = pd.DataFrame({'customer_id': [3, 4, 5, 6]})

# Convert columns to sets
set1 = set(df1['customer_id'])
set2 = set(df2['customer_id'])

# Find common customers (intersection)
common_customers = set1 & set2
print("Common customers:", common_customers)

# Find unique customers in df1 (difference)
unique_to_df1 = set1 - set2
print("Unique to df1:", unique_to_df1)

Handling Missing Values:
# Sample DataFrame with missing values
df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]})

# Find rows with missing values in any column
missing_indices = set(df[df.isnull().any(axis=1)].index)
print("Rows with missing values:", missing_indices)

Merging Datasets:
# Sample DataFrames
df1 = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})
df2 = pd.DataFrame({'customer_id': [3, 4, 5], 'name': ['Charlie', 'David', 'Eve']})

# Merge DataFrames using set intersection
merged_df = pd.merge(df1, df2, on='customer_id', how='inner')
print("Merged DataFrame on common customers:\n", merged_df)

Skill Improvement: Using Statistical Libraries in Python
Using SciPy and Statsmodels for Calculations and Visualizations
SciPy:
SciPy is a Python library used for scientific and technical computing. It provides modules for statistics, optimization, integration, and more.

Example Code:

Calculating Measures of Central Tendency:
import numpy as np
from scipy import stats

data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]

# Mean
mean = np.mean(data)
print("Mean:", mean)

# Median
median = np.median(data)
print("Median:", median)

# Mode
mode = stats.mode(data)
print("Mode:", mode.mode[0])

Calculating Measures of Dispersion:
# Range
data_range = np.ptp(data)
print("Range:", data_range)

# Variance
variance = np.var(data)
print("Variance:", variance)

# Standard Deviation
std_dev = np.std(data)
print("Standard Deviation:", std_dev)

Probability Calculations:
# Basic Probability Example
from fractions import Fraction

favorable_outcomes = 1  # e.g., probability of rolling a 6 on a die
total_outcomes = 6
probability = Fraction(favorable_outcomes, total_outcomes)
print("Probability of rolling a 6:", float(probability))

Visualizing Data:
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.hist(data, bins=5, edgecolor='black')
plt.title('Histogram')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

# Box Plot
sns.boxplot(data)
plt.title('Box Plot')
plt.show()

# Scatter Plot
x = np.arange(len(data))
y = data
plt.scatter(x, y)
plt.title('Scatter Plot')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models

pip install pandas numpy matplotlib seaborn

Import Libraries:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Load the Dataset:
df = pd.read_csv('loan_approval_data.csv')

Inspect the Dataset:
print(df.head())
print(df.info())

Analyze Demographic Distribution:
gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyze Outcome Distribution:
approval_by_gender = df.groupby('gender')['loan_approved'].mean()
approval_by_race = df.groupby('race')['loan_approved'].mean()

print(approval_by_gender)
print(approval_by_race)

sns.barplot(x='gender', y='loan_approved', data=df)
plt.show()

sns.barplot(x='race', y='loan_approved', data=df)
plt.show()

Calculate Bias Metrics:
from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')
8.Handson Activities
Practical Exercises
Identifying Bias in Datasets
Objective: Learn how to identify and measure bias in datasets to ensure fair and unbiased AI models.
Step by Step Guide:
1. Set Up the Environment:
    Install Python and necessary libraries:

pip install pandas numpy matplotlib seaborn

2. Import Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

3. Load the Dataset:
    For this example, we’ll use a hypothetical dataset that includes demographic information and outcomes (e.g., loan approval data):

df = pd.read_csv('loan_approval_data.csv')

4. Inspect the Dataset:

print(df.head())
print(df.info())

5. Analyse Demographic Distribution:
    Check the distribution of different demographic groups (e.g., gender, race) in the dataset:

gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()

Analyse Outcome Distribution:
    Check the distribution of outcomes (e.g., loan approvals) across different demographic groups:

gender_distribution = df['gender'].value_counts()
race_distribution = df['race'].value_counts()

print(gender_distribution)
print(race_distribution)

sns.countplot(x='gender', data=df)
plt.show()

sns.countplot(x='race', data=df)
plt.show()


7. Calculate Bias Metrics:
    Use fairness metrics to measure bias in the dataset:


from sklearn.metrics import confusion_matrix

def calculate_disparate_impact(group, target):
    group_positive_rate = df[df[group] == 1][target].mean()
    group_negative_rate = df[df[group] == 0][target].mean()
    disparate_impact = group_positive_rate / group_negative_rate
    return disparate_impact

gender_disparate_impact = calculate_disparate_impact('gender', 'loan_approved')
race_disparate_impact = calculate_disparate_impact('race', 'loan_approved')

print(f'Gender Disparate Impact: {gender_disparate_impact}')
print(f'Race Disparate Impact: {race_disparate_impact}')

Creating Fair AI Models
Objective: Develop AI models that mitigate bias and ensure fair outcomes across different demographic groups.
Step by Step Guide:
1. Set Up the Environment:
    Install Python and necessary libraries:


pip install pandas numpy scikitlearn aif360

2. Import Libraries:

import pandas as pd
   import numpy as np
   from sklearn.model_selection import train_test_split
   from sklearn.linear_model import LogisticRegression
   from sklearn.metrics import accuracy_score, confusion_matrix
   from aif360.datasets import BinaryLabelDataset
   from aif360.metrics import BinaryLabelDatasetMetric
   from aif360.algorithms.preprocessing import Reweighing

3. Load and Preprocess the Dataset


df = pd.read_csv('loan_approval_data.csv')
df['gender'] = df['gender'].map({'male': 1, 'female': 0})
df['race'] = df['race'].map({'white': 1, 'non_white': 0})

X = df.drop('loan_approved', axis=1)
y = df['loan_approved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


4. Train a Baseline Model


model = LogisticRegression()
   model.fit(X_train, y_train)

   y_pred = model.predict(X_test)
   print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
   print(confusion_matrix(y_test, y_pred))

5. Assess Bias in the Model


test_dataset = BinaryLabelDataset(df=pd.concat([X_test, y_test], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
metric = BinaryLabelDatasetMetric(test_dataset, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])

print(f'Disparate Impact (Gender): {metric.disparate_impact()}')
print(f'Disparate Impact (Race): {metric.disparate_impact()}')

6. Mitigate Bias Using Reweighing


RW = Reweighing(unprivileged_groups=[{'gender': 0}, {'race': 0}], privileged_groups=[{'gender': 1}, {'race': 1}])
train_dataset = BinaryLabelDataset(df=pd.concat([X_train, y_train], axis=1), label_names=['loan_approved'], protected_attribute_names=['gender', 'race'])
train_dataset_transf = RW.fit_transform(train_dataset)

model.fit(train_dataset_transf.features, train_dataset_transf.labels.ravel())
y_pred_transf = model.predict(X_test)

print(f'Accuracy after Reweighing: {accuracy_score(y_test, y_pred_transf)}')
print(confusion_matrix(y_test, y_pred_transf))

test_dataset_pred_transf = test_dataset.copy(deepcopy=True)
test_dataset_pred_transf.labels = y_pred_transf

metric_transf = BinaryLabelDatasetMetric(test_dataset_pred_transf, privileged_groups=[{'gender': 1}, {'race': 1}], unprivileged_groups=[{'gender': 0}, {'race': 0}])
print(f'Disparate Impact after Reweighing (Gender): {metric_transf.disparate_impact()}')
print(f'Disparate Impact after Reweighing (Race): {metric_transf.disparate_impact()}')

8. Handson Activities
Creating Frequency Tables and Graphs
Objective: Learn how to create frequency tables and corresponding graphs to summarize and visualize data.

3. Creating Graphs:
    Use bar graphs or pie charts to visualize the frequency table

import matplotlib.pyplot as plt

# Data
fruits = ['Apple', 'Banana', 'Orange']
frequency = [4, 3, 3]

# Bar Graph
plt.bar(fruits, frequency, color=['red', 'yellow', 'orange'])
plt.xlabel('Fruit')
plt.ylabel('Frequency')
plt.title('Fruit Frequency')
plt.show()

# Pie Chart
plt.pie(frequency, labels=fruits, colors=['red', 'yellow', 'orange'], autopct='%1.1f%%')
plt.title('Fruit Frequency Distribution')
plt.show()

Calculating Statistical Measures (Mean, Median, Mode, Range, etc.)
Objective: Calculate key statistical measures to summarize the central tendency and dispersion of a dataset.

import numpy as np
from scipy import stats
data = [15, 18, 20, 22, 25, 25, 30]
# Mean
mean = np.mean(data)
print(f"Mean: {mean}")
# Median
median = np.median(data)
print(f"Median: {median}")
# Mode
mode = stats.mode(data)
print(f"Mode: {mode.mode[0]}")
# Range
data_range = np.ptp(data)
print(f"Range: {data_range}")
# Variance
variance = np.var(data)
print(f"Variance: {variance}")
# Standard Deviation
std_deviation = np.std(data)
print(f"Standard Deviation: {std_deviation}")

Expected Output:

Mean: 22.142857142857142
Median: 22.0
Mode: 25
Range: 15
Variance: 24.97959183673469
Standard Deviation: 4.997958786252244

Objective: Develop the ability to interpret and analyse data visualizations and statistical summaries.

import matplotlib.pyplot as plt
# Box Plot
plt.boxplot(data)
plt.title('Box Plot of Data')
plt.show()
# Histogram
plt.hist(data, bins=5, color='blue', edgecolor='black')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram of Data')
plt.show()
# Scatter Plot (for paired data, e.g., height and weight)
heights = [150, 160, 165, 170, 175]
weights = [50, 60, 62, 70, 75]
plt.scatter(heights, weights, color='green')
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.title('Scatter Plot of Height vs Weight')
plt.show()


2. Plot Data Points:
    
Python Code to Create a Scatterplot:

import matplotlib.pyplot as plt
# Data
hours_studied = [1, 2, 3, 4, 5]
exam_scores = [50, 55, 65, 70, 75]
# Create Scatterplot
plt.scatter(hours_studied, exam_scores, color='blue')
plt.xlabel('Hours Studied')
plt.ylabel('Exam Scores')
plt.title('Scatterplot of Hours Studied vs. Exam Scores')
plt.show()

Visual Representation
Plotting the Line of Best Fit:
Python Code 

import matplotlib.pyplot as plt
import numpy as np

# Data
hours_studied = [1, 2, 3, 4, 5]
exam_scores = [50, 55, 65, 70, 75]

# Scatterplot
plt.scatter(hours_studied, exam_scores, color='blue')

# Line of Best Fit
slope = 6.5
intercept = 43.5
x = np.array(hours_studied)
y = slope * x + intercept
plt.plot(x, y, color='red')

plt.xlabel('Hours Studied')
plt.ylabel('Exam Scores')
plt.title('Scatterplot with Line of Best Fit')
plt.show()

2. Calculate Pearson’s r:

Python Code to Calculate Pearson’s r:

import numpy as np

# Data
hours_studied = np.array([2, 3, 5, 6, 8])
exam_scores = np.array([56, 59, 70, 72, 80])

# Calculate Pearson's r
correlation_matrix = np.corrcoef(hours_studied, exam_scores)
pearson_r = correlation_matrix[0, 1]
print(f"Pearson's r: {pearson_r}")

Expected Output:
Pearson's r: 0.9877

2. Build the Regression Model

import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
# Reshape data
hours_studied = hours_studied.reshape(-1, 1)
exam_scores = exam_scores.reshape(-1, 1)
# Create and fit the model
model = LinearRegression()
model.fit(hours_studied, exam_scores)
# Predict exam scores
predicted_scores = model.predict(hours_studied)
# Plot data and regression line
plt.scatter(hours_studied, exam_scores, color='blue', label='Actual Scores')
plt.plot(hours_studied, predicted_scores, color='red', label='Regression Line')
plt.xlabel('Hours Studied')
plt.ylabel('Exam Scores')
plt.title('Simple Linear Regression')
plt.legend()
plt.show()
# Print slope and intercept
slope = model.coef_[0][0]
intercept = model.intercept_[0]
print(f"Slope: {slope}, Intercept: {intercept}")

Expected Output:
Slope: 3.9286, Intercept: 48.8

Objective: Evaluate the regression model's performance using various metrics and interpret the results.
Steps:
1. Calculate Evaluation Metrics:

from sklearn.metrics import mean_squared_error, r2_score

# Calculate MSE
mse = mean_squared_error(exam_scores, predicted_scores)
rmse = np.sqrt(mse)
r_squared = r2_score(exam_scores, predicted_scores)

print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"R²: {r_squared}")

Expected Output:
MSE: 4.96
RMSE: 2.23
R²: 0.9756


Projects on Regression
Applying Regression to Real world Datasets

Python Code for EDA and Model Building

import pandas as pd
from sklearn.model_selection import train_test_split
# Load dataset
data = pd.read_csv('house_prices.csv')
# EDA
plt.scatter(data['Square_Footage'], data['Price'], color='blue')
plt.xlabel('Square Footage')
plt.ylabel('Price')
plt.title('Square Footage vs. Price')
plt.show()
# Split data
X = data[['Square_Footage']]
y = data['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Build and train the model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict and evaluate
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r_squared = r2_score(y_test, y_pred)
print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"R²: {r_squared}")

2. Prediction:
    
Python Example:

import numpy as np
from sklearn.linear_model import LogisticRegression
# Sample data
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
y = np.array([0, 0, 0, 1, 1])
# Create and train the model
model = LogisticRegression()
model.fit(X, y)
# Predict probabilities
probabilities = model.predict_proba(X)
print("Probabilities:\n", probabilities)
# Predict classes
predictions = model.predict(X)
print("Predictions:\n", predictions)

Examples of Clustering Problems
Example:
A retail company wants to segment its customers based on their purchasing behaviour to create targeted marketing campaigns.
Python Example (KMeans Clustering)

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
# Sample data
data = {
    'Customer ID': [1, 2, 3, 4],
    'Purchase Frequency': [10, 5, 15, 20],
    'Average Purchase Value': [50, 100, 30, 20],
    'Age': [25, 35, 28, 45],
    'Income': [30000, 50000, 40000, 60000]
}
df = pd.DataFrame(data)
# Features for clustering
X = df[['Purchase Frequency', 'Average Purchase Value', 'Age', 'Income']]
# K-Means Clustering
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
df['Cluster'] = kmeans.labels_
# Plotting
plt.scatter(df['Purchase Frequency'], df['Average Purchase Value'], c=df['Cluster'], cmap='viridis')
plt.xlabel('Purchase Frequency')
plt.ylabel('Average Purchase Value')
plt.title('Customer Segmentation')
plt.show()


Texture patterns
Dataset:
An image can be represented as a matrix of pixel values.

import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
# Load image
image = cv2.imread('image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
# Reshape the image to a 2D array of pixels
pixels = image.reshape(-1, 3)
# K-Means Clustering
kmeans = KMeans(n_clusters=3)
kmeans.fit(pixels)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape).astype(np.uint8)
# Plotting
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(image)
plt.subplot(1, 2, 2)
plt.title('Segmented Image')
plt.imshow(segmented_img)
plt.show()

Algorithm:
1. Initialization: Select K initial centroids randomly from the dataset.
2. Assignment: Assign each data point to the nearest centroid, forming K clusters.
3. Update: Recalculate the centroids by computing the mean of all data points in each cluster.
4. Repeat: Repeat the assignment and update steps until the centroids no longer change or the maximum number of iterations is reached.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])

# K-Means Clustering
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.show()

1. Elbow Method:
    Plot the sum of squared distances (inertia) against the number of clusters.
    Look for an "elbow" point where the rate of decrease slows down, indicating the optimal K


inertia = []
K = range(1, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(K, inertia, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

2. Silhouette Score:
    Measures how similar a data point is to its own cluster compared to other clusters.
    Silhouette scores range from 1 to 1, with higher values indicating better clustering.


from sklearn.metrics import silhouette_score

silhouette_scores = []
K = range(2, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)
    score = silhouette_score(X, kmeans.labels_)
    silhouette_scores.append(score)

plt.plot(K, silhouette_scores, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Optimal K')
plt.show()

2. Divisive (Top Down) Approach:
    Start with all data points in a single cluster.
    Recursively split clusters into smaller clusters until each data point is its own cluster or a desired number of clusters is reached.

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Sample data
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])

# Hierarchical Clustering
Z = linkage(X, 'ward')

# Plotting Dendrogram
plt.figure()
dendrogram(Z)
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

Python Example:

from sklearn.metrics import silhouette_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])

# K-Means Clustering
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
labels = kmeans.labels_

# Silhouette Score
score = silhouette_score(X, labels)
print(f'Silhouette Score: {score}')

Elbow Method:
Steps:
1. Compute KMeans clustering for different values of K (e.g., 1 to 10).
2. Plot the inertia for each K.
3. Identify the point where the curve bends (elbow point).
Python Example:
# Elbow Method
inertia = []
K = range(1, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(K, inertia, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

Practical Exercises
 Building and Evaluating a Binary Classification Model
Objective: Build and evaluate a binary classification model using logistic regression.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
# Load dataset (example: breast cancer dataset from sklearn)
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X = data.data
y = data.target
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)
# Predict on test data
y_pred = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')


Implementing Logistic Regression
Objective: Implement logistic regression and interpret the model coefficients.
Python Example:

# Model coefficients
coefficients = model.coef_[0]
intercept = model.intercept_[0]

print("Model coefficients:", coefficients)
print("Model intercept:", intercept)

# Logistic regression equation
print(f'Logistic regression equation: logit(P) = {intercept} + {coefficients[0]}*X1 + {coefficients[1]}*X2 + ... + {coefficients[-1]}*Xn')


Constructing and Analysing a Confusion Matrix
Objective: Construct and analyse a confusion matrix to evaluate the performance of a binary classification model.
Python Example:


# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)

# Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()


Python Example:

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
# Sample data (example: iris dataset from sklearn)
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
# K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_
# Plotting
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.show()
# Silhouette Score
from sklearn.metrics import silhouette_score
score = silhouette_score(X, labels)
print(f'Silhouette Score: {score}')

Projects on Classification and Clustering
Developing a Classification Model for a Real world Dataset

# Example using a customer churn dataset (you would replace this with the actual dataset you're using)
import pandas as pd
# Load dataset
data = pd.read_csv('customer_churn.csv')
# Preprocess data
# (This step would include handling missing values, encoding categorical variables, standardizing features, etc.)
# Split data
from sklearn.model_selection import train_test_split
X = data.drop('Churn', axis=1)
y = data['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)
# Predict on test data
y_pred = model.predict(X_test)
# Evaluate the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')
print("Confusion Matrix:")
print(cm)
# Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

Applying Clustering to a Dataset and Analysing the Results
Objective: Apply clustering to a real world dataset and analyse the results.

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
# Load dataset
data = pd.read_csv('customer_segmentation.csv')
# Preprocess data  # (This step would include handling missing values, standardizing features, etc.)
# K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(data)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_
# Plotting
plt.scatter(data['Feature1'], data['Feature2'], c=labels, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.show()
# Silhouette Score
from sklearn.metrics import silhouette_score
score = silhouette_score(data, labels)
print(f'Silhouette Score: {score}')
# Elbow Method
inertia = []
K = range(1, 10)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data)
    inertia.append(kmeans.inertia_)
plt.plot(K, inertia, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

3. Feature Analysis: Evaluate features to identify potential sources of bias. Ensure that sensitive attributes (e.g., race, gender) are not unfairly influencing the outcomes.
Python Example:

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('dataset.csv')

# Descriptive statistics
print(data.groupby('gender')['income'].describe())

# Visualization
sns.histplot(data, x='income', hue='gender', multiple='stack')
plt.title('Income Distribution by Gender')
plt.show()

# Correlation analysis
correlation_matrix = data.corr()
print(correlation_matrix['income'])


Evaluating Model Outputs
Definition:
Evaluating model outputs involves assessing the predictions made by AI models to ensure they are fair and unbiased across different groups.


from sklearn.metrics import confusion_matrix, precision_score, recall_score
import numpy as np

# Load test data and predictions
y_test = np.array([0, 1, 1, 0, 1])
y_pred = np.array([0, 1, 0, 0, 1])
sensitive_attribute = np.array(['male', 'female', 'female', 'male', 'female'])

# Evaluate model performance metrics by group
for group in np.unique(sensitive_attribute):
    indices = np.where(sensitive_attribute == group)
    print(f'Performance for {group}:')
    print('Confusion Matrix:')
    print(confusion_matrix(y_test[indices], y_pred[indices]))
    print('Precision:', precision_score(y_test[indices], y_pred[indices]))
    print('Recall:', recall_score(y_test[indices], y_pred[indices]))

# Calculate fairness metrics (e.g., demographic parity)
# Note: Actual fairness metric calculations would require additional steps and definitions

Handson Examples
Identifying Bias in Datasets
Example 1: Gender Bias in Income Data
Python Example:

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('income_data.csv')

# Descriptive statistics
print(data.groupby('gender')['income'].describe())

# Visualization
sns.boxplot(x='gender', y='income', data=data)
plt.title('Income Distribution by Gender')
plt.show()

Example 2: Bias in a Predictive Policing System
Python Example:
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score
# Load dataset
data = pd.read_csv('crime_data.csv')
y_true = data['actual_crime']
y_pred = data['predicted_crime']
demographic_group = data['demographic']
# Evaluate performance by demographic group
for group in data['demographic'].unique():
    indices = data[data['demographic'] == group].index
    print(f'Performance for {group}:')
    print('Confusion Matrix:')
    print(confusion_matrix(y_true[indices], y_pred[indices]))
    print('Accuracy:', accuracy_score(y_true[indices], y_pred[indices]))
# Analyze bias
# Example: Check if one demographic group has significantly higher false positive rates

Python Example:

from sklearn.utils import resample

# Example of under-sampling
majority_class = data[data['class'] == 'majority']
minority_class = data[data['class'] == 'minority']

# Down-sample majority class
majority_downsampled = resample(majority_class, 
                                replace=False, 
                                n_samples=len(minority_class), 
                                random_state=123)

# Combine minority class with down-sampled majority class
data_balanced = pd.concat([majority_downsampled, minority_class])

# Check new class distribution
print(data_balanced['class'].value_counts())


2. Re-weighting: Assigning different weights to samples to correct imbalances.
    Increase the weights of underrepresented groups during model training.
Python Example:

# Example using scikit-learn's LogisticRegression with class_weight parameter
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight='balanced')
model.fit(X_train, y_train)

3. Feature Engineering: Removing or modifying biased features that could lead to unfair outcomes.
    Exclude sensitive attributes like race, gender, etc., from the model input.
Python Example:

# Drop sensitive attribute
X = data.drop(columns=['sensitive_attribute'])

4. Data Anonymization: Removing personally identifiable information (PII) to prevent discrimination based on identity.
Python Example:

# Example of anonymizing data by removing PII
data_anonymized = data.drop(columns=['name', 'address', 'SSN'])

Algorithmic Fairness
.
Python Example:

from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds
from fairlearn.metrics import demographic_parity_difference

# Example of using Fairlearn to apply equalized odds constraint
constraint = EqualizedOdds()
model = ExponentiatedGradient(base_estimator, constraints=constraint)
model.fit(X_train, y_train, sensitive_features=sensitive_attribute)

2. Fair Algorithms: Using algorithms specifically designed to be fairer.
    Fair decision trees, fair kmeans clustering, etc.
Python Example:

from fairlearn.postprocessing import ThresholdOptimizer

# Example of using Fairlearn's ThresholdOptimizer
postproc = ThresholdOptimizer(estimator=model, constraints="demographic_parity", prefit=True)
postproc.fit(X_train, y_train, sensitive_features=sensitive_attribute)

3. Adversarial Debiasing: Using adversarial networks to reduce bias.
    Train an adversarial network to identify and remove bias inducing features.
Python Example:
# Example: Setup adversarial debiasing (pseudo-code)
# 1. Train a model to predict the target variable.
# 2. Train an adversarial model to predict the sensitive attribute from the residuals.
# 3. Adjust the original model to minimize both the prediction error and the adversary's ability to detect the sensitive attribute.

Inclusive AI Design
Definition:
Inclusive AI design involves involving diverse teams and stakeholders in the AI development process to ensure the system is fair and considers a wide range of perspectives and needs.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds
# Load dataset
data = pd.read_csv('dataset.csv')
# Balance the dataset
majority_class = data[data['class'] == 'majority']
minority_class = data[data['class'] == 'minority']
majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=123)
data_balanced = pd.concat([majority_downsampled, minority_class])
# Remove sensitive attribute
X = data_balanced.drop(columns=['sensitive_attribute'])
y = data_balanced['class']
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train a fair algorithm
base_estimator = LogisticRegression()
constraint = EqualizedOdds()
model = ExponentiatedGradient(base_estimator, constraints=constraint)
model.fit(X_train, y_train, sensitive_features=data_balanced['sensitive_attribute'])

7. Handson Activities
Practical Exercises
Detecting and Analysing Bias in Sample Datasets
Python Example:

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load sample dataset (example: adult income dataset)
data = pd.read_csv('adult.csv')

# Inspect the dataset
print(data.head())
print(data.info())

# Descriptive statistics
print(data.groupby('sex')['income'].describe())

# Visualization of income distribution by gender
sns.countplot(x='income', hue='sex', data=data)
plt.title('Income Distribution by Gender')
plt.show()

# Correlation analysis
# Encode categorical variables
label_enc = LabelEncoder()
data['sex'] = label_enc.fit_transform(data['sex'])
data['income'] = label_enc.fit_transform(data['income'])

correlation_matrix = data.corr()
print(correlation_matrix['income'])

Implementing Bias Mitigation Strategies in AI Models
Objective: Implement various bias mitigation strategies, such as resampling, reweighting, and fairness constraints in AI models.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds
# Load and preprocess dataset (example: adult income dataset)
data = pd.read_csv('adult.csv')
data['sex'] = label_enc.fit_transform(data['sex'])
data['income'] = label_enc.fit_transform(data['income'])
# Balance the dataset
majority_class = data[data['income'] == 0]
minority_class = data[data['income'] == 1]
majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=123)
data_balanced = pd.concat([majority_downsampled, minority_class])
# Split data
X = data_balanced.drop(columns=['income'])
y = data_balanced['income']
sensitive_attribute = data_balanced['sex']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train a fair algorithm
base_estimator = LogisticRegression(max_iter=10000)
constraint = EqualizedOdds()
model = ExponentiatedGradient(base_estimator, constraints=constraint)
model.fit(X_train, y_train, sensitive_features=sensitive_attribute)
# Evaluate the model
y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

Projects on Ethical AI
Developing an AI Model with Bias Detection and Mitigation

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds
import seaborn as sns
import matplotlib.pyplot as plt
# Load dataset (example: credit scoring dataset)
data = pd.read_csv('credit_data.csv')
# Detect bias
print(data.groupby('gender')['loan_approved'].describe())
sns.countplot(x='loan_approved', hue='gender', data=data)
plt.title('Loan Approval by Gender')
plt.show()


# Preprocess data
data['gender'] = label_enc.fit_transform(data['gender'])
data['loan_approved'] = label_enc.fit_transform(data['loan_approved'])
# Balance the dataset
majority_class = data[data['loan_approved'] == 0]
minority_class = data[data['loan_approved'] == 1]
majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=123)
data_balanced = pd.concat([majority_downsampled, minority_class])
# Split data
X = data_balanced.drop(columns=['loan_approved'])
y = data_balanced['loan_approved']
sensitive_attribute = data_balanced['gender']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train a fair algorithm
base_estimator = LogisticRegression(max_iter=10000)
constraint = EqualizedOdds()
model = ExponentiatedGradient(base_estimator, constraints=constraint)
model.fit(X_train, y_train, sensitive_features=sensitive_attribute)
# Evaluate the model
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

Presenting Case Studies on AI Bias and Its Mitigation
Objective: Research and present case studies on AI bias, its impacts, and the mitigation strategies implemented to address it.

# Case Study Presentation: Amazon's AI Recruiting Tool
## Introduction
- Overview of Amazon's AI recruiting tool and its purpose.
## Bias Source and Impact
- Historical data reflecting gender bias.
- Discriminatory hiring practices against female candidates.
## Mitigation Strategies
- Amazon discontinued the tool.
- Increased human oversight in the hiring process.
## Outcomes and Lessons Learned
- Importance of representative training data.
- Need for continuous monitoring and evaluation.
## Conclusion and Recommendations
- Regular audits of AI systems for bias.
- Diverse teams in AI development to ensure different perspectives are considered.























































